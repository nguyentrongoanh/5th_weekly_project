{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "df_train = pd.read_csv('train.csv')\n",
    "df_test = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign Sentiment and reviews column \n",
    "reviews = df_train['Phrase']\n",
    "sentiment = df_train['Sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([<matplotlib.axis.XTick at 0x7fbcb9779fd0>,\n",
       "  <matplotlib.axis.XTick at 0x7fbcb9786b50>,\n",
       "  <matplotlib.axis.XTick at 0x7fbcb9786750>,\n",
       "  <matplotlib.axis.XTick at 0x7fbcbab1ec50>,\n",
       "  <matplotlib.axis.XTick at 0x7fbcbab331d0>],\n",
       " [Text(0, 0, ''),\n",
       "  Text(0, 0, ''),\n",
       "  Text(0, 0, ''),\n",
       "  Text(0, 0, ''),\n",
       "  Text(0, 0, '')])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# visualize sentiment data\n",
    "fig = plt.figure()\n",
    "axes1 = fig.add_subplot(1, 1, 1)\n",
    "axes1.hist(sentiment)\n",
    "axes1.set_xlabel('Sentiment')\n",
    "axes1.set_ylabel('Count')\n",
    "plt.xticks(range(5))\n",
    "# Most of the review is \"neutral\", what prediction i can make base on this data? \n",
    "# Maybe it shows that the data is unbalance so we cannot evaluate the model base on accuracy?\n",
    "# or most of words in the phrase represent for the neutral sentiment?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"disguise the slack complacency of -LRB- Godard 's -RRB- vision\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count words in the review\n",
    "vocab = Counter()\n",
    "for phrase in reviews:\n",
    "    for word in phrase.split():\n",
    "        vocab[word] += 1\n",
    "\n",
    "# Quick look at the most common word\n",
    "vocab.most_common()\n",
    "\n",
    "# There is some strange word like: -RRB-, what does it mean?\n",
    "# No result from google, must find the context where this word is in.\n",
    "# Luckily, it's in the second phrase of the dataset\n",
    "reviews.loc[1]\n",
    "# Combine with the result on google it's the LEFT ROUND BRACKET and RIGHT ROUND BRACKET, should get rid of them\n",
    "# There're also digit character in the data, not much information from this digit, get rid of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to remove -LRB- or -RRB-\n",
    "def remove_brackets(text):\n",
    "    for word in text.split():\n",
    "        if word == '-LRB-' or word == '-RRB-':\n",
    "            text = text.replace(word, '')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove -LRB- or -RRB- in reviews\n",
    "reviews = reviews.apply(remove_brackets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to remove special character and digits (reuse from Quaan's code)\n",
    "import re\n",
    "def preprocessor(text):\n",
    "    \"\"\" Return a cleaned version of text\n",
    "    \"\"\"\n",
    "    # Remove HTML markup\n",
    "    text = re.sub('<[^>]*>', '', text)\n",
    "    # Save emoticons for later appending\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text)\n",
    "    # Remove any non-word character and append the emoticons,\n",
    "    # Also Convert to lower case\n",
    "#     text = (re.sub('[0-9]', ' ', text.lower()) + ' ' + ' '.join(emoticons).replace('-', ''))\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove special character and digits\n",
    "# reviews = reviews.apply(preprocessor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download list of stopwords\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stopwords\n",
    "def remove_stopwords(text):\n",
    "    filtered_word = []\n",
    "    for word in text.split(' '):\n",
    "        if word not in stop_words:\n",
    "            filtered_word.append(word)\n",
    "    return ' '.join(filtered_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stopwords before tokenizer to avoid warning while stemming\n",
    "reviews = reviews.apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for stemming (reuse Quan's code)\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "porter = PorterStemmer()\n",
    "\n",
    "# Split a text into list of words and apply stemming technic\n",
    "def tokenizer_porter(text):\n",
    "    return [porter.stem(word) for word in text.split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Logistic Regression using tfidf\n",
    "X_train = reviews\n",
    "y_train = sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('vect',\n",
       "                 TfidfVectorizer(preprocessor=<function preprocessor at 0x7fbcb7b518c0>,\n",
       "                                 tokenizer=<function tokenizer_porter at 0x7fbcb7b51050>)),\n",
       "                ('clf', LogisticRegression(max_iter=10000))])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transforms data into matrix\n",
    "tfidf = TfidfVectorizer(tokenizer=tokenizer_porter,\n",
    "                        preprocessor=preprocessor)\n",
    "# instantiate estimator\n",
    "clf = Pipeline([('vect', tfidf),\n",
    "                ('clf', LogisticRegression(max_iter=10000))])\n",
    "\n",
    "# training model\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.6754496133965568\n",
      "confusion matrix:\n",
      " [[ 1270  2457  1372   143    11]\n",
      " [  406  9262  9997   854    44]\n",
      " [  147  2696 53234  3392   136]\n",
      " [   31   562 10310 13139   736]\n",
      " [    6    71  1041  3575  2153]]\n",
      "classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.24      0.36      5253\n",
      "           1       0.62      0.45      0.52     20563\n",
      "           2       0.70      0.89      0.79     59605\n",
      "           3       0.62      0.53      0.57     24778\n",
      "           4       0.70      0.31      0.43      6846\n",
      "\n",
      "    accuracy                           0.68    117045\n",
      "   macro avg       0.66      0.49      0.53    117045\n",
      "weighted avg       0.67      0.68      0.65    117045\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluation on training set\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "# Now apply those above metrics to evaluate your model\n",
    "# Your code here\n",
    "predictions = clf.predict(X_train)\n",
    "print('accuracy:',accuracy_score(y_train,predictions))\n",
    "print('confusion matrix:\\n',confusion_matrix(y_train,predictions))\n",
    "print('classification report:\\n',classification_report(y_train,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing test data\n",
    "X_test = df_test['Phrase']\n",
    "X_test = X_test.apply(remove_brackets)\n",
    "X_test = X_test.apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 2, ..., 2, 2, 3])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prediction on test_set\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "\n",
    "# Now apply those above metrics to evaluate your model\n",
    "# Your code here\n",
    "predictions = clf.predict(X_test)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add result in test_set\n",
    "df_test['Sentiment'] = predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export to csv\n",
    "df_test.to_csv('submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = clf.predict_proba(X_test)\n",
    "\n",
    "for i in range(len(X_test)):\n",
    "    print(f'{X_test[i]} --> 0, 1, 2, 3, 4, 5 = {preds[i]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
